{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBLINK :- \n",
    "First website – (https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained )\n",
    "\n",
    "Second Website – (https://builtin.com/artificial-intelligence )\n",
    "\n",
    "Third website – (https://www.infoworld.com/article/3634602/explainable-ai-explained.html )\n",
    "\n",
    "Fourth Website- (https://www2.deloitte.com/se/sv/pages/technology/articles/part1-artificial-intelligence-defined.html )\n",
    "\n",
    "Fifth Website – (https://www.ibm.com/in-en/watson/explainable-ai )\n",
    "\n",
    "\n",
    "# PROBLEM DEFINITON :- \n",
    "Imagine you are working as a data scientist in an AI-based organization. You have to build A web scrapping model implementing word2vec which will throw similar words to the word you are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Basic Python Libraries\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt   \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in d:\\python\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Installing Beautifulsoup \n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in d:\\python\\lib\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# Installing lxml file : lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language\n",
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\python\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\python\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\python\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\python\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Installing nltk Packages\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nltk.download' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download Packages for nltk\n",
    "!nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request\n",
    "import re\n",
    "scraped_data =  urllib.request.urlopen('https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained')\n",
    "article = scraped_data.read()\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "para = parsed_article.find_all('p')\n",
    "article_text = \"\"\n",
    "for p in para:\n",
    "  article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Text Cleaning\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]',' ',processed_article)\n",
    "processed_article = re.sub(r'\\s+',' ',processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of Dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range (len(all_words)):\n",
    "  all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'machine': 0, 'learning': 1, 'data': 2, 'program': 3, 'said': 4, 'use': 5, 'like': 6, 'intelligence': 7, 'ai': 8, 'people': 9, 'learn': 10, 'way': 11, 'business': 12, 'humans': 13, 'artificial': 14, 'companies': 15, 'used': 16, 'example': 17, 'mit': 18, 'models': 19, 'different': 20, 'machines': 21, 'time': 22, 'language': 23, 'trained': 24, 'model': 25, 'shulman': 26, 'work': 27, 'algorithms': 28, 'computers': 29, 'human': 30, 'algorithm': 31, 'neural': 32, 'need': 33, 'management': 34, 'understand': 35, 'would': 36, 'processing': 37, 'information': 38, 'madry': 39, 'computer': 40, 'potential': 41, 'using': 42, 'malone': 43, 'mba': 44, 'things': 45, 'natural': 46, 'leaders': 47, 'make': 48, 'engineering': 49, 'problems': 50, 'find': 51, 'world': 52, 'system': 53, 'chatbots': 54, 'identify': 55, 'able': 56, 'networks': 57, 'likely': 58, 'network': 59, 'technology': 60, 'one': 61, 'deep': 62, 'well': 63, 'cases': 64, 'degree': 65, 'medical': 66, 'text': 67, 'accurate': 68, 'content': 69, 'cars': 70, 'look': 71, 'found': 72, 'perform': 73, 'year': 74, 'whether': 75, 'patterns': 76, 'much': 77, 'new': 78, 'company': 79, 'good': 80, 'images': 81, 'researchers': 82, 'take': 83, 'read': 84, 'uses': 85, 'future': 86, 'behind': 87, 'google': 88, 'working': 89, 'important': 90, 'director': 91, 'systems': 92, 'might': 93, 'sloan': 94, 'image': 95, 'research': 96, 'understanding': 97, 'nodes': 98, 'layers': 99, 'training': 100, 'pictures': 101, 'field': 102, 'executives': 103, 'center': 104, 'done': 105, 'even': 106, 'action': 107, 'transactions': 108, 'explicitly': 109, 'ability': 110, 'tasks': 111, 'get': 112, 'create': 113, 'train': 114, 'including': 115, 'recognize': 116, 'also': 117, 'according': 118, 'scientist': 119, 'task': 120, 'tell': 121, 'features': 122, 'facebook': 123, 'instead': 124, 'normally': 125, 'programs': 126, 'impact': 127, 'picture': 128, 'meaning': 129, 'many': 130, 'determine': 131, 'another': 132, 'career': 133, 'mid': 134, 'trying': 135, 'solve': 136, 'changing': 137, 'science': 138, 'analyze': 139, 'tools': 140, 'self': 141, 'driving': 142, 'finance': 143, 'businesses': 144, 'problem': 145, 'accuracy': 146, 'two': 147, 'car': 148, 'made': 149, 'several': 150, 'netflix': 151, 'every': 152, 'decisions': 153, 'autonomous': 154, 'social': 155, 'show': 156, 'making': 157, 'ways': 158, 'best': 159, 'based': 160, 'unsupervised': 161, 'right': 162, 'vehicles': 163, 'limitations': 164, 'labeled': 165, 'know': 166, 'search': 167, 'translate': 168, 'suggestions': 169, 'subfield': 170, 'ideas': 171, 'gives': 172, 'professor': 173, 'month': 174, 'matter': 175, 'basic': 176, 'next': 177, 'terms': 178, 'value': 179, 'full': 180, 'sometimes': 181, 'think': 182, 'powers': 183, 'without': 184, 'industry': 185, 'programmed': 186, 'non': 187, 'master': 188, 'predictive': 189, 'thinking': 190, 'professionals': 191, 'survey': 192, 'collective': 193, 'expertise': 194, 'everyone': 195, 'technical': 196, 'leading': 197, 'earn': 198, 'today': 199, 'watch': 200, 'aware': 201, 'ethical': 202, 'still': 203, 'others': 204, 'though': 205, 'core': 206, 'deal': 207, 'great': 208, 'diagnostics': 209, 'works': 210, 'face': 211, 'recognition': 212, 'indicates': 213, 'cat': 214, 'performing': 215, 'output': 216, 'cell': 217, 'cells': 218, 'brain': 219, 'modeled': 220, 'assistants': 221, 'digital': 222, 'putting': 223, 'answers': 224, 'never': 225, 'languages': 226, 'customers': 227, 'question': 228, 'occupation': 229, 'taken': 230, 'x': 231, 'makes': 232, 'speakers': 233, 'voice': 234, 'amazon': 235, 'avoid': 236, 'practice': 237, 'bias': 238, 'enough': 239, 'older': 240, 'tend': 241, 'tuberculosis': 242, 'actually': 243, 'success': 244, 'experts': 245, 'explainability': 246, 'tool': 247, 'certain': 248, 'vary': 249, 'detection': 250, 'ads': 251, 'us': 252, 'twitter': 253, 'want': 254, 'recommendation': 255, 'conversations': 256, 'millions': 257, 'thousands': 258, 'pioneer': 259, 'records': 260, 'numbers': 261, 'focus': 262, 'difficult': 263, 'easily': 264, 'follow': 265, 'requires': 266, 'amount': 267, 'amounts': 268, 'programming': 269, 'traditional': 270, 'written': 271, 'programmers': 272, 'csail': 273, 'head': 274, 'complex': 275, 'intelligent': 276, 'defined': 277, 'better': 278, 'really': 279, 'co': 280, 'larovere': 281, 'going': 282, 'engage': 283, 'sales': 284, 'programmer': 285, 'lots': 286, 'supervised': 287, 'noted': 288, 'helps': 289, 'reinforcement': 290, 'clients': 291, 'types': 292, 'online': 293, 'could': 294, 'looking': 295, 'trends': 296, 'common': 297, 'dogs': 298, 'predict': 299, 'help': 300, 'function': 301, 'brief': 302, 'recent': 303, 'wrote': 304, 'successful': 305, 'sets': 306, 'shown': 307, 'chicken': 308, 'generate': 309, 'happened': 310, 'results': 311, 'open': 312}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index \n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simillar_words = word2vec.wv.most_similar('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('intelligent', 0.3780343532562256), ('research', 0.3102955222129822), ('two', 0.3044637441635132), ('traditional', 0.28415074944496155), ('mit', 0.2783488929271698), ('languages', 0.2631463408470154), ('machine', 0.2556622624397278), ('work', 0.25370389223098755), ('limitations', 0.25335612893104553), ('companies', 0.2513876259326935)]\n"
     ]
    }
   ],
   "source": [
    "print(simillar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
